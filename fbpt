#!/usr/bin/env python3
import argparse
import base64
import fnmatch
import getpass
import hashlib
import json
import math
import os
import re
import shutil
import sys
import zipfile
from collections import defaultdict
from datetime import datetime
from enum import Enum

##################################################################
#                            Constant                            #
##################################################################
CLASSIFICATION_RULES = [
    {'type': 'time', 'granularity': 'year/month'},
    {'type': 'file_type'},
    {'type': 'size'}
]
FILE_CATEGORIES = {
    'IMAGE_EXTENSIONS': ['.jpg', '.jpeg', '.png', '.heic', '.gif'],
    'VIDEO_EXTENSIONS': ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv', '.m4v', '.mpg', '.mpeg'],
    'AUDIO_EXTENSIONS': ['.mp3', '.wav', '.m4a'],
    'TEXT_EXTENSIONS': ['.txt', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.pdf',
                        '.rtf', '.odt', '.md'],
    'ARCHIVE_EXTENSIONS': ['.rar', '.7z', '.zip', '.gz', '.xz', '.tar'],
    'CODE_EXTENSIONS': ['.c', '.cpp', '.cc', '.cxx', '.cp', '.c++', '.h', '.hpp', '.hxx', '.java', '.kt',
                        '.scala', '.sc', '.py', '.rs', '.rb', '.php', '.html', '.htm', '.css', '.js',
                        '.ts', '.sh', '.bat'],
    'DATA_EXTENSIONS': ['.json', '.csv', '.dat'],
    'CONFIG_EXTENSIONS': ['.yml', '.yaml', '.xml', '.ini']
}

CONVERSION_MAP = {
    'image': {
        'extensions': ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff', '.webp'],
        'target_ext': '.png'
    },
    'pdf_to_docx': {
        'extensions': ['.pdf'],
        'target_ext': '.docx'
    },
    'word_to_pdf': {
        'extensions': ['.doc', '.docx'],
        'target_ext': '.pdf'
    },
    'excel_to_csv': {
        'extensions': ['.xls', '.xlsx'],
        'target_ext': '.csv'
    },
    'md_to_html': {
        'extensions': ['.md'],
        'target_ext': '.html'
    }
}

COLOR_RED = '\033[91m'
COLOR_GREEN = '\033[92m'
COLOR_YELLOW = '\033[93m'
COLOR_RESET = '\033[0m'

PIP = 'pip' if sys.platform == 'win32' else 'pip3'

DEFAULT_CHUNK_SIZE = 512 * 1024  # 512 KiB


class DeduplicateAction(Enum):
    DELETE = 'delete'
    MOVE = 'move'


class ExportFormat(Enum):
    JSON = 'json'
    CSV = 'csv'
    DEFAULT = 'default'


class HashAlgorithm(Enum):
    MD5 = 'md5'
    SHA256 = 'sha256'


##################################################################
#                            Utility                             #
##################################################################
def check_dir(dirname):
    if not os.path.exists(dirname):
        print_error(f'Error: {dirname} does not exist!')
        exit(1)

    if not os.path.isdir(dirname):
        print_error(f'Error: {dirname} is not a directory.')
        exit(1)


def makedirs_if_not_exists(dirname):
    if not os.path.exists(dirname):
        os.makedirs(dirname)


def mkdir_if_not_exists(dirname):
    if not os.path.exists(dirname):
        os.mkdir(dirname)


def check_src_and_make_dest(src_dir, dest_dir):
    check_dir(src_dir)
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir)
    elif not os.path.isdir(dest_dir):
        print_error(f'Error: {dest_dir} is not a directory.')
        exit(1)


def get_chunk_number(filepath, chunk_size):
    file_size = os.path.getsize(filepath)
    return (file_size + chunk_size - 1) // chunk_size


def print_success(message):
    print(f'{COLOR_GREEN}{message}{COLOR_RESET}')


def print_warning(message):
    print(f'{COLOR_YELLOW}{message}{COLOR_RESET}')


def print_error(message):
    print(f'{COLOR_RED}{message}{COLOR_RESET}')


def classify_by_time(filepath, time_format):
    mtime = os.path.getmtime(filepath)
    return datetime.fromtimestamp(mtime).strftime(time_format)


def classify_by_size(filepath):
    file_size = os.path.getsize(filepath)
    if file_size < 1024 * 1024:
        return '0-1M'
    if file_size < 100 * 1024 * 1024:
        return '1M-100M'
    if file_size < 1024 * 1024 * 1024:
        return '100M-1G'
    return '1G+'


def classify_by_file_type(filepath):
    _, ext = os.path.splitext(filepath)
    ext = ext.lower()
    for category, exts in FILE_CATEGORIES.items():
        if ext in exts:
            return category.split('_')[0].lower()
    return 'others'


def get_file_hash(filepath, hash_algorithm=HashAlgorithm.MD5.value):
    if hash_algorithm == HashAlgorithm.MD5.value:
        hasher = hashlib.md5()
    else:
        hasher = hashlib.sha256()

    with open(filepath, 'rb') as f:
        num_chunk = get_chunk_number(filepath, DEFAULT_CHUNK_SIZE)
        for i in range(num_chunk):
            chunk_data = f.read(DEFAULT_CHUNK_SIZE)
            hasher.update(chunk_data)
    return hasher.hexdigest()


def calculate_entropy(filepath):
    file_size = os.path.getsize(filepath)
    if file_size == 0:
        return 0.0

    counts = defaultdict(int)
    try:
        with open(filepath, 'rb') as f:
            num_chunk = get_chunk_number(filepath, DEFAULT_CHUNK_SIZE)
            for i in range(num_chunk):
                chunk_data = f.read(DEFAULT_CHUNK_SIZE)
                for byte in chunk_data:
                    counts[byte] += 1
    except Exception as e:
        print_error(f'Error reading {filepath}: {str(e)}')
        return 0.0

    entropy = 0.0
    for count in counts.values():
        p = count / file_size
        entropy -= p * math.log2(p)
    return entropy


def derive_key(password: str, salt: bytes) -> bytes:
    try:
        from cryptography.hazmat.primitives import hashes
        from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    except ImportError:
        print_error('Error: No module named cryptography. '
                    f'Please run "{PIP} install cryptography".')
        exit(1)

    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,
        salt=salt,
        iterations=100000,
    )
    return base64.urlsafe_b64encode(kdf.derive(password.encode()))


def encrypt_file(input_path, output_path, password):
    try:
        from cryptography.fernet import Fernet
    except ImportError:
        print_error('Error: No module named cryptography. '
                    f'Please run "{PIP} install cryptography".')
        exit(1)

    salt = os.urandom(16)
    key = derive_key(password, salt)
    fernet = Fernet(key)

    with open(output_path, 'wb') as outfile:
        outfile.write(salt)
        with open(input_path, 'rb') as infile:
            num_chunk = get_chunk_number(input_path, DEFAULT_CHUNK_SIZE)
            for i in range(num_chunk):
                chunk_data = infile.read(DEFAULT_CHUNK_SIZE)
                encrypted_chunk_data = fernet.encrypt(chunk_data)
                outfile.write(encrypted_chunk_data)


def decrypt_file(input_path, output_path, password):
    try:
        from cryptography.fernet import Fernet
    except ImportError:
        print_error('Error: No module named cryptography. '
                    f'Please run "{PIP} install cryptography".')
        exit(1)

    with open(input_path, 'rb') as infile:
        salt = infile.read(16)
        key = derive_key(password, salt)
        fernet = Fernet(key)

        remaining_size = os.path.getsize(input_path) - 16
        num_chunk = (remaining_size + DEFAULT_CHUNK_SIZE - 1) // DEFAULT_CHUNK_SIZE
        with open(output_path, 'wb') as outfile:
            for i in range(num_chunk):
                encrypted_chunk_data = infile.read(DEFAULT_CHUNK_SIZE)
                try:
                    decrypted_chunk_data = fernet.decrypt(encrypted_chunk_data)
                    outfile.write(decrypted_chunk_data)
                except Exception:
                    print_error('Error: invalid password or corrupted file.')
                    exit(1)


def convert_image_format(source_file, target_file, target_format='PNG'):
    try:
        from PIL import Image
    except ImportError:
        print_error('Error: No module named PIL. '
                    f'Please run "{PIP} install Pillow".')
        exit(1)

    try:
        with Image.open(source_file) as img:
            img.save(target_file, format=target_format)
        print(f'Converted image {source_file} to {target_format} format and saved as {target_file}.')
    except Exception as e:
        print_error(f'Failed to convert image {source_file} to {target_format}: {str(e)}')


def pdf_to_docx(source_file, target_file):
    try:
        from pdf2docx import Converter
    except ImportError:
        print_error('Error: No module named pdf2docx. '
                    f'Please run "{PIP} install pdf2docx".')
        exit(1)

    try:
        cv = Converter(source_file)
        cv.convert(target_file, start=0, end=None)
        cv.close()
        print(f'Converted PDF {source_file} to Docx.')
    except Exception as e:
        print_error(f'Failed to convert PDF {source_file} to Docx: {str(e)}')


def word_to_pdf(source_file, target_file):
    try:
        from win32com import client as wc
    except ImportError:
        print_error('Error: No module named win32com. '
                    f'Please run "{PIP} install pywin32".')
        exit(1)

    try:
        word = wc.Dispatch('Word.Application')
        doc = word.Documents.Open(os.path.abspath(source_file))
        doc.SaveAs(os.path.abspath(target_file), FileFormat=17)
        doc.Close()
        word.Quit()
        print(f'Converted Word {source_file} to PDF.')
    except Exception as e:
        print_error(f'Failed to convert Word {source_file} to PDF: {str(e)}')


def excel_to_csv(source_file, target_file):
    try:
        import pandas as pd
    except ImportError:
        print_error('Error: No module named pandas. '
                    f'Please run "{PIP} install pandas".')
        exit(1)

    try:
        df = pd.read_excel(source_file)
        df.to_csv(target_file, index=False)
        print(f'Converted Excel {source_file} to CSV.')
    except Exception as e:
        print_error(f'Failed to convert Excel {source_file} to CSV: {str(e)}')


def markdown_to_html(source_file, target_file):
    try:
        import markdown
    except ImportError:
        print_error('Error: No module named markdown. '
                    f'Please run "{PIP} install markdown".')
        exit(1)

    try:
        with open(source_file, 'r', encoding='utf-8') as f:
            text = f.read()
        html = markdown.markdown(text)
        with open(target_file, 'w', encoding='utf-8') as f:
            f.write(html)
        print(f'Converted Markdown {source_file} to HTML.')
    except Exception as e:
        print_error(f'Failed to convert Markdown {source_file} to HTML: {str(e)}')


##################################################################
#                    Batch Processing Method                     #
##################################################################
def batch_classify_files(rootdir, rules):
    check_dir(rootdir)
    for root, _, files in os.walk(rootdir):
        for filename in files:
            src = os.path.join(root, filename)
            if not os.path.isfile(src):
                continue
            path_parts = []
            for rule in rules:
                if rule['type'] == 'time':
                    path_parts.append(classify_by_time(src, rule['format']))
                elif rule['type'] == 'size':
                    path_parts.append(classify_by_size(src))
                elif rule['type'] == 'file_type':
                    path_parts.append(classify_by_file_type(src))
            dest_dir = os.path.join(rootdir, *path_parts)
            os.makedirs(dest_dir, exist_ok=True)
            dest = os.path.join(dest_dir, filename)
            if src != dest:
                try:
                    shutil.move(src, dest)
                except shutil.Error:
                    base, extension = os.path.splitext(filename)
                    counter = 1
                    while True:
                        new_name = f'{base}_{counter}{extension}'
                        new_path = os.path.join(dest_dir, new_name)
                        if not os.path.exists(new_path):
                            shutil.move(src, new_path)
                            break
                        counter += 1


def batch_rename_files(rootdir, prefix, recursive=False, pattern=None, verbose=False):
    check_dir(rootdir)
    idx = 0
    for filename in os.listdir(rootdir):
        if pattern and not re.search(pattern, filename, re.IGNORECASE):
            continue
        _, ext = os.path.splitext(filename)
        new_name = os.path.join(rootdir, f'{prefix}_{idx}{ext}')
        old_name = os.path.join(rootdir, filename)
        if recursive and os.path.isdir(old_name):
            batch_rename_files(old_name, prefix, recursive=recursive, pattern=pattern, verbose=verbose)
        if verbose:
            print(f'replace \'{filename}\' with \'{new_name}\'')
        os.rename(old_name, new_name)
        idx += 1


def batch_create_files(rootdir, count, prefix, ext=None):
    makedirs_if_not_exists(rootdir)
    for i in range(count):
        filename = f'{prefix}_{i}'
        if ext:
            filename += ext
        pathname = os.path.join(rootdir, filename)
        open(pathname, 'w+').close()


def batch_mkdir(rootdir, count, prefix, level=1):
    makedirs_if_not_exists(rootdir)
    dirs = [rootdir]
    for cur_level in range(level):
        new_dirs = []
        for dirname in dirs:
            for idx in range(count):
                cur_prefix = prefix if cur_level == 0 else os.path.basename(dirname)
                dirpath = os.path.join(dirname, f'{cur_prefix}_{idx}')
                mkdir_if_not_exists(dirpath)
                new_dirs.append(dirpath)
        dirs = new_dirs


def batch_remove_files(rootdir, recursive=False, pattern=None, verbose=False):
    batch_remove_files_internal(rootdir, recursive=recursive, pattern=pattern, verbose=verbose)
    if recursive and len(os.listdir(rootdir)) == 0:
        if verbose:
            print(f'Remove rootdir: {rootdir}')
        os.rmdir(rootdir)


def batch_remove_files_internal(rootdir, recursive=False, pattern=None, verbose=False):
    check_dir(rootdir)
    for filename in os.listdir(rootdir):
        pathname = os.path.join(rootdir, filename)
        match_pattern = pattern and re.search(pattern, pathname)
        if os.path.isdir(pathname):
            if not recursive:
                continue
            if not pattern or match_pattern:
                if verbose:
                    print(f'Remove dir: {pathname}')
                shutil.rmtree(pathname)
            else:
                batch_remove_files_internal(pathname, recursive=recursive, pattern=pattern, verbose=verbose)
        else:
            if not pattern or match_pattern:
                if verbose:
                    print(f'Remove file: {pathname}')
                os.remove(pathname)


def batch_write_files(rootdir, content,
                      append=False, recursive=False,
                      pattern=None, verbose=False):
    check_dir(rootdir)
    mode = 'a' if append else 'w'
    for filename in os.listdir(rootdir):
        pathname = os.path.join(rootdir, filename)
        if os.path.isdir(pathname):
            if recursive:
                batch_write_files(pathname, content,
                                  append=append, recursive=recursive,
                                  pattern=pattern, verbose=verbose)
        else:
            if pattern and not re.search(pattern, filename):
                continue
            if verbose:
                print(f'Write file: {pathname}')
            with open(pathname, mode) as file:
                file.write(content)


def batch_copy_files(src_dir, dest_dir, pattern=None, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    for filename in os.listdir(src_dir):
        if pattern and not re.search(pattern, filename):
            continue
        old_path_name = os.path.join(src_dir, filename)
        new_path_name = os.path.join(dest_dir, filename)
        if os.path.isdir(old_path_name):
            if os.path.exists(new_path_name):
                print_warning(f'Destination path \'{new_path_name}\' already exists')
            else:
                if verbose:
                    print(f'Copy dir: {old_path_name}')
                shutil.copytree(old_path_name, new_path_name)
        else:
            if verbose:
                print(f'Copy file: {old_path_name}')
            shutil.copyfile(old_path_name, new_path_name)


def batch_move_files(src_dir, dest_dir, pattern=None, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    any_match = False
    for filename in os.listdir(src_dir):
        if pattern and not re.search(pattern, filename):
            continue
        any_match = True
        src_path = os.path.join(src_dir, filename)
        dest_path = os.path.join(dest_dir, filename)
        if os.path.exists(dest_path):
            print_warning(f'Destination path \'{dest_path}\' already exists.')
        else:
            if verbose:
                print(f'Move \'{src_path}\' to \'{dest_path}\'')
            shutil.move(src_path, dest_dir)
    if not any_match:
        print(f'None of the paths match the pattern: {pattern}')


def batch_split_files_by_line(src_dir, dest_dir, line_count, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    for filename in os.listdir(src_dir):
        if filename[0] == '.':
            continue
        pathname = os.path.join(src_dir, filename)
        with open(pathname, 'r') as file:
            chunk_id = 0
            chunk_data = file.readlines(line_count)
            while chunk_data:
                chunk_path_name = os.path.join(dest_dir, f'{filename}_chunk{chunk_id}')
                with open(chunk_path_name, 'w+') as chunk_file:
                    if verbose:
                        print(f'split: {pathname}, chunk id: {chunk_id}')
                    chunk_file.writelines(chunk_data)
                chunk_id += 1
                chunk_data = file.readlines(line_count)


def batch_split_files_by_byte(src_dir, dest_dir, chunk_size, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    for filename in os.listdir(src_dir):
        if filename[0] == '.':
            continue
        pathname = os.path.join(src_dir, filename)
        file_size = os.path.getsize(pathname)
        chunk_number = (file_size + chunk_size - 1) // chunk_size
        with open(pathname, 'rb') as file:
            for chunk_id in range(chunk_number):
                chunk_data = file.read(chunk_size)
                chunk_pathname = os.path.join(dest_dir, f'{filename}_chunk{chunk_id}')
                with open(chunk_pathname, 'wb') as chunk_file:
                    if verbose:
                        print(f'split: {pathname}, chunk id: {chunk_id}')
                    chunk_file.write(chunk_data)


def batch_merge_files(src_dir, output_file, pattern=None, verbose=False):
    check_dir(src_dir)

    def file_sort_order(filename):
        res = filename.split('chunk')
        if len(res) > 1:
            return res[0], int(res[1])
        return res[0], ''

    with open(output_file, 'wb') as outfile:
        for filename in sorted(os.listdir(src_dir), key=file_sort_order):
            if filename[0] == '.':
                continue
            if pattern and not re.search(pattern, filename, re.IGNORECASE):
                continue
            pathname = os.path.join(src_dir, filename)
            if verbose:
                print(f'merge {pathname}')
            with open(pathname, 'rb') as infile:
                num_chunk = get_chunk_number(pathname, DEFAULT_CHUNK_SIZE)
                for i in range(num_chunk):
                    outfile.write(infile.read(DEFAULT_CHUNK_SIZE))


def batch_chmod(rootdir, mode, recursive=False, pattern=None, verbose=False):
    check_dir(rootdir)
    for filename in os.listdir(rootdir):
        if pattern and not re.search(pattern, filename, re.IGNORECASE):
            continue
        pathname = os.path.join(rootdir, filename)
        if recursive and os.path.isdir(pathname):
            batch_chmod(pathname, mode, recursive=recursive, pattern=pattern, verbose=verbose)
        if verbose:
            print(f'chmod {mode} {pathname}')
        os.chmod(pathname, int(mode, 8))


def batch_find_files(rootdir, shell_pattern, regular_expression):
    check_dir(rootdir)
    assert shell_pattern or regular_expression
    for dirpath, _, filenames in os.walk(rootdir):
        for filename in filenames:
            pathname = os.path.join(dirpath, filename)
            if shell_pattern:
                if fnmatch.fnmatch(filename, shell_pattern):
                    print(pathname)
            else:
                if re.search(regular_expression, filename, re.IGNORECASE):
                    print(pathname)


def batch_check_file_name(rootdir, pattern):
    check_dir(rootdir)
    all_match = True
    for dirpath, _, filenames in os.walk(rootdir):
        for filename in filenames:
            pathname = os.path.join(dirpath, filename)
            if not re.fullmatch(pattern, filename, re.IGNORECASE):
                print(f'{pathname} does not match!')
                all_match = False
    if all_match:
        print_success('PASS')


def batch_check_file_integrity(rootdir, input_hash_file,
                               hash_algorithm=HashAlgorithm.MD5.value):
    check_dir(rootdir)

    expected_file_hash_dict = {}
    with open(input_hash_file, 'r') as file:
        for line in file:
            line = line.strip()
            file_path, file_hash = line.split(' = ')
            expected_file_hash_dict[file_path] = file_hash

    file_paths = []
    for dirpath, _, filenames in os.walk(rootdir):
        for filename in filenames:
            file_paths.append(os.path.join(os.path.abspath(dirpath), filename))

    for file_path in file_paths:
        if file_path not in expected_file_hash_dict:
            continue

        expected_file_hash = expected_file_hash_dict[file_path]
        real_file_hash = get_file_hash(file_path, hash_algorithm=hash_algorithm)

        if expected_file_hash != real_file_hash:
            print(f'mismatch: {file_path} (expected: {expected_file_hash}, real: {real_file_hash})')



def batch_deduplicate(rootdir, action=DeduplicateAction.DELETE.value, dry_run=False):
    check_dir(rootdir)
    hashes = defaultdict(list)
    total_saved = 0
    for root, _, files in os.walk(rootdir):
        for filename in files:
            filepath = os.path.join(root, filename)
            if os.path.isfile(filepath):
                hash_val = get_file_hash(filepath)
                hashes[hash_val].append(filepath)
    for hash_value, files in hashes.items():
        if len(files) > 1:
            print_warning(f'duplicate file (MD5: {hash_value[:8]}...)')
            keep_file = files[0]
            duplicates = files[1:]
            for dup in duplicates:
                if dry_run:
                    print(f'[dry_run] process: {dup}')
                    continue
                if action == DeduplicateAction.DELETE.value:
                    size_dup = os.path.getsize(dup)
                    os.remove(dup)
                    total_saved += size_dup
                elif action == DeduplicateAction.MOVE.value:
                    dup_dir = os.path.join(rootdir, 'duplicates')
                    os.makedirs(dup_dir, exist_ok=True)
                    shutil.move(dup, os.path.join(dup_dir, os.path.basename(dup)))
                print(f'{action}: {dup}')
    if action == DeduplicateAction.DELETE.value:
        print(f'Free up space: {total_saved / 1024 / 1024:.2f} MB')


def batch_hash(rootdir, hash_algorithm=HashAlgorithm.MD5.value,
               output_file=None, output_format=ExportFormat.CSV.value,
               recursive=False, shell_pattern=None, regular_expression=None,
               verbose=False):
    check_dir(rootdir)

    file_hash_values = {}
    batch_hash_internal(rootdir, file_hash_values,
                        hash_algorithm=hash_algorithm,
                        recursive=recursive,
                        shell_pattern=shell_pattern,
                        regular_expression=regular_expression,
                        verbose=verbose)
    if output_file:
        with open(output_file, 'w') as outfile:
            if output_format == ExportFormat.JSON.value:
                json.dump(file_hash_values, outfile, indent=4)
            elif output_format == ExportFormat.CSV.value:
                outfile.write(f'filepath, {hash_algorithm}\n')
                for filepath, hash_value in file_hash_values.items():
                    outfile.write(f'{filepath}, {hash_value}\n')
            else:
                for filepath, hash_value in file_hash_values.items():
                    outfile.write(f'{filepath} = {hash_value}\n')
    else:
        for filepath, hash_value in file_hash_values.items():
            print(f'{hash_algorithm} ({filepath}) = {hash_value}')


def batch_hash_internal(rootdir, hash_values,
                        hash_algorithm=HashAlgorithm.MD5.value, recursive=False,
                        shell_pattern=None, regular_expression=None,
                        verbose=False):
    for filename in os.listdir(rootdir):
        pathname = os.path.join(os.path.abspath(rootdir), filename)
        if os.path.isdir(pathname):
            if recursive:
                batch_hash_internal(pathname, hash_values,
                                    hash_algorithm=hash_algorithm,
                                    recursive=recursive,
                                    shell_pattern=shell_pattern,
                                    regular_expression=regular_expression,
                                    verbose=verbose)
        else:
            if shell_pattern:
                if not fnmatch.fnmatch(filename, shell_pattern):
                    continue
            elif regular_expression:
                if not re.match(regular_expression, filename, re.IGNORECASE):
                    continue

            if verbose:
                print(f'Calculate: {pathname}')
            hash_values[pathname] = get_file_hash(pathname, hash_algorithm=hash_algorithm)


def batch_compress_files(rootdir, threshold=7.0, remove_original=False, verbose=False):
    skip_compress_extensions = FILE_CATEGORIES.get('ARCHIVE_EXTENSIONS')
    check_dir(rootdir)
    for root, _, files in os.walk(rootdir):
        for filename in files:
            src = os.path.join(root, filename)
            if not os.path.isfile(src):
                continue
            if any(filename.lower().endswith(ext) for ext in skip_compress_extensions):
                continue
            entropy = calculate_entropy(src)
            if entropy < threshold:
                if verbose:
                    print(f'Compressing {src} (entropy: {entropy:.2f})')
                zip_path = f'{src}.zip'
                try:
                    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                        zf.write(src, arcname=filename)
                except Exception as e:
                    print_error(f'Compression failed for {src}: {str(e)}')
                    continue
                if remove_original:
                    try:
                        os.remove(src)
                    except Exception as e:
                        print_error(f'Failed to remove {src}: {str(e)}')


def batch_encrypt_files(rootdir, password, pattern=None, verbose=False):
    check_dir(rootdir)
    for root, _, files in os.walk(rootdir):
        for filename in files:
            if pattern and not fnmatch.fnmatch(filename, pattern):
                continue
            input_path = os.path.join(root, filename)
            if filename.endswith('.enc'):
                continue
            output_path = f'{input_path}.enc'
            try:
                encrypt_file(input_path, output_path, password)
                if verbose:
                    print(f'Encrypted {input_path} -> {output_path}')
            except Exception as e:
                print_error(f'Encryption failed for {input_path}: {str(e)}')


def batch_decrypt_files(rootdir, password, pattern=None, verbose=False):
    check_dir(rootdir)
    for root, _, files in os.walk(rootdir):
        for filename in files:
            if pattern and not fnmatch.fnmatch(filename, pattern):
                continue
            if not filename.endswith('.enc'):
                continue
            input_path = os.path.join(root, filename)
            output_path = input_path[:-4]
            try:
                decrypt_file(input_path, output_path, password)
                if verbose:
                    print(f'Decrypted {input_path} -> {output_path}')
            except Exception as e:
                print_error(f'Decryption failed for {input_path}: {str(e)}')


def batch_convert_files(src_dir, dest_dir, conversion_type, format='PNG', recursive=False):
    check_src_and_make_dest(src_dir, dest_dir)
    for root, dirs, files in os.walk(src_dir):
        if not recursive and root != src_dir:
            continue
        for file in files:
            source_file = os.path.join(root, file)
            file_ext = os.path.splitext(file)[1].lower()
            config = CONVERSION_MAP.get(conversion_type)
            if config is None:
                print_error(f'Unsupported conversion type: {conversion_type}')
                return
            if file_ext in config['extensions']:
                target_file_name = os.path.splitext(file)[0] + config['target_ext']
                target_file = os.path.join(dest_dir, target_file_name)
                print(f'Converting {source_file} to {target_file} using {conversion_type}')
                if conversion_type == 'image':
                    convert_image_format(source_file, target_file, format)
                elif conversion_type == 'pdf_to_docx':
                    pdf_to_docx(source_file, target_file)
                elif conversion_type == 'word_to_pdf':
                    word_to_pdf(source_file, target_file)
                elif conversion_type == 'excel_to_csv':
                    excel_to_csv(source_file, target_file)
                elif conversion_type == 'md_to_html':
                    markdown_to_html(source_file, target_file)


##################################################################
#                       Subcommand Handler                       #
##################################################################
def handle_classify(args):
    rules = []
    for rule in args.rules:
        if rule.startswith('time:'):
            _, fmt = rule.split(':', 1)
            rules.append({'type': 'time', 'format': fmt})
        elif rule == 'size':
            rules.append({'type': 'size'})
        elif rule == 'file_type':
            rules.append({'type': 'file_type'})
        else:
            print_error(f'Invalid rule: {rule}')
            exit(1)
    batch_classify_files(args.rootdir, rules)


def handle_rename(args):
    batch_rename_files(
        args.rootdir, args.prefix, recursive=args.recursive,
        pattern=args.pattern, verbose=args.verbose
    )


def handle_create(args):
    batch_create_files(
        args.rootdir, args.count, args.prefix, ext=args.ext
    )


def handle_mkdir(args):
    batch_mkdir(
        args.rootdir, args.count, args.prefix, level=args.level
    )


def handle_remove(args):
    batch_remove_files(
        args.rootdir, recursive=args.recursive,
        pattern=args.pattern, verbose=args.verbose
    )


def handle_write(args):
    batch_write_files(
        args.rootdir, content=args.content, append=args.append,
        recursive=args.recursive, pattern=args.pattern,
        verbose=args.verbose
    )


def handle_copy(args):
    batch_copy_files(
        args.src, args.dest, pattern=args.pattern,
        verbose=args.verbose
    )


def handle_move(args):
    batch_move_files(
        args.src, args.dest, pattern=args.pattern,
        verbose=args.verbose
    )


def handle_split(args):
    if args.line:
        batch_split_files_by_line(
            args.src, args.dest, args.line,
            verbose=args.verbose
        )
    else:
        batch_split_files_by_byte(
            args.src, args.dest, args.chunk_size,
            verbose=args.verbose
        )


def handle_merge(args):
    batch_merge_files(
        args.src, args.output, pattern=args.pattern,
        verbose=args.verbose
    )


def handle_chmod(args):
    batch_chmod(
        args.rootdir, args.mode, recursive=args.recursive,
        pattern=args.pattern, verbose=args.verbose
    )


def handle_find(args):
    batch_find_files(args.rootdir, args.pattern, args.expression)


def handle_check_file_name(args):
    batch_check_file_name(args.rootdir, args.pattern)


def handle_check_file_integrity(args):
    batch_check_file_integrity(args.rootdir, args.input, args.hash_algorithm)


def handle_deduplicate(args):
    batch_deduplicate(
        args.rootdir, action=args.action, dry_run=args.dry_run
    )


def handle_hash(args):
    if args.csv:
        output_format = ExportFormat.CSV.value
    elif args.json:
        output_format = ExportFormat.JSON.value
    else:
        output_format = ExportFormat.DEFAULT.value
    batch_hash(args.rootdir, hash_algorithm=args.hash_algorithm,
               output_file=args.output, output_format=output_format,
               recursive=args.recursive,
               shell_pattern=args.pattern, regular_expression=args.expression,
               verbose=args.verbose)


def handle_compress(args):
    batch_compress_files(
        args.rootdir,
        threshold=args.threshold,
        remove_original=args.remove,
        verbose=args.verbose
    )


def handle_encrypt(args):
    password = getpass.getpass('Encryption password: ')
    while not password or not password.strip():
        print_warning('Invalid password')
        password = getpass.getpass('Encryption password: ')

    batch_encrypt_files(
        args.rootdir,
        password,
        pattern=args.pattern,
        verbose=args.verbose
    )


def handle_decrypt(args):
    password = getpass.getpass('Decryption password: ')
    while not password or not password.strip():
        print_warning('Invalid password')
        password = getpass.getpass('Decryption password: ')

    batch_decrypt_files(
        args.rootdir,
        password,
        pattern=args.pattern,
        verbose=args.verbose
    )


def handle_convert(args):
    batch_convert_files(
        args.src, args.dest, conversion_type=args.type,
        format=args.format.upper(), recursive=args.recursive
    )


##################################################################
#                              Main                              #
##################################################################
def main():
    parser = argparse.ArgumentParser(
        description='File batch processing tool (fbpt)'
    )
    subparsers = parser.add_subparsers(
        dest='subcommand', help='subcommand'
    )

    # classify options
    classify_parser = subparsers.add_parser(
        'classify', help='Classify files into directory structure'
    )
    classify_parser.add_argument(
        '-r', '--rootdir', required=True,
        help='Root directory to process'
    )
    classify_parser.add_argument(
        '--rules', required=True, nargs='+',
        help=(
            'Classification rules (format: type[:param]) \n'
            'Available types: \n'
            '  time:strftime_format (e.g. time:%%Y/%%m) \n'
            '  size \n'
            '  file_type'
        )
    )

    # rename options
    rename_parser = subparsers.add_parser(
        'rename', help='Batch rename file'
    )
    rename_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    rename_parser.add_argument(
        '-p', '--prefix', type=str, required=True,
        help='File name prefix'
    )
    rename_parser.add_argument(
        '-P', '--pattern', type=str, help='Pattern'
    )
    rename_parser.add_argument(
        '-R', '--recursive', action='store_true',
        help='Recursive traversal'
    )
    rename_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # create options
    create_parser = subparsers.add_parser(
        'create', help='Batch create file'
    )
    create_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    create_parser.add_argument(
        '-c', '--count', type=int, required=True,
        help='Number of files/dirs'
    )
    create_parser.add_argument(
        '-p', '--prefix', type=str, required=True,
        help='File name prefix'
    )
    create_group = create_parser.add_mutually_exclusive_group()
    create_group.add_argument(
        '-m', '--mkdir', action='store_true',
        help='Create directory (default creates file)'
    )
    create_group.add_argument(
        '-e', '--ext', type=str,
        help='File extension'
    )

    # mkdir options
    mkdir_parser = subparsers.add_parser(
        'mkdir', help='Batch create directory'
    )
    mkdir_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    mkdir_parser.add_argument(
        '-c', '--count', type=int, required=True,
        help='Number of dirs'
    )
    mkdir_parser.add_argument(
        '-l', '--level', type=int, default=1,
        help='Directory Level'
    )
    mkdir_parser.add_argument(
        '-p', '--prefix', type=str, required=True,
        help='File name prefix'
    )

    # remove options
    remove_parser = subparsers.add_parser(
        'remove', help='Batch remove file'
    )
    remove_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    remove_parser.add_argument(
        '-R', '--recursive', action='store_true',
        help='Recursive traversal'
    )
    remove_parser.add_argument(
        '-p', '--pattern', type=str,
        help='Pattern'
    )
    remove_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # write options
    write_parser = subparsers.add_parser(
        'write', help='Batch write file'
    )
    write_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    write_parser.add_argument(
        '-c', '--content', type=str, required=True,
        help='File content'
    )
    write_parser.add_argument(
        '-a', '--append', action='store_true',
        help='Append mode (By default, overwrite mode is used.)'
    )
    write_parser.add_argument(
        '-R', '--recursive', action='store_true',
        help='Recursive traversal'
    )
    write_parser.add_argument(
        '-p', '--pattern', type=str,
        help='Pattern'
    )
    write_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # copy options
    copy_parser = subparsers.add_parser(
        'copy', help='Batch copy file'
    )
    copy_parser.add_argument(
        '-s', '--src', type=str, required=True,
        help='Source directory'
    )
    copy_parser.add_argument(
        '-d', '--dest', type=str, required=True,
        help='Destination directory'
    )
    copy_parser.add_argument(
        '-p', '--pattern', type=str,
        help='Pattern'
    )
    copy_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # move options
    move_parser = subparsers.add_parser(
        'move', help='Batch move file'
    )
    move_parser.add_argument(
        '-s', '--src', type=str, required=True,
        help='Source directory'
    )
    move_parser.add_argument(
        '-d', '--dest', type=str, required=True,
        help='Destination directory'
    )
    move_parser.add_argument(
        '-p', '--pattern', type=str,
        help='Pattern'
    )
    move_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # split options
    split_parser = subparsers.add_parser(
        'split', help='Batch split file'
    )
    split_parser.add_argument(
        '-s', '--src', type=str, required=True,
        help='Source directory'
    )
    split_parser.add_argument(
        '-d', '--dest', type=str, required=True,
        help='Destination directory'
    )
    split_group = split_parser.add_mutually_exclusive_group(required=True)
    split_group.add_argument(
        '-l', '--line', type=int,
        help='Line count'
    )
    split_group.add_argument(
        '-c', '--chunk-size', type=int,
        help='Chunk Size (in byte)'
    )
    split_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # merge options
    merge_parser = subparsers.add_parser(
        'merge', help='Batch merge file'
    )
    merge_parser.add_argument(
        '-s', '--src', type=str, required=True,
        help='Source directory'
    )
    merge_parser.add_argument(
        '-o', '--output', type=str, required=True,
        help='Output file'
    )
    merge_parser.add_argument(
        '-p', '--pattern', type=str,
        help='Pattern'
    )
    merge_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # chmod options
    chmod_parser = subparsers.add_parser(
        'chmod', help='Batch chmod'
    )
    chmod_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    chmod_parser.add_argument(
        '-m', '--mode', type=str, required=True,
        help='Mode'
    )
    chmod_parser.add_argument(
        '-R', '--recursive', action='store_true',
        help='Recursive traversal'
    )
    chmod_parser.add_argument(
        '-p', '--pattern', type=str,
        help='Pattern'
    )
    chmod_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # find options
    find_parser = subparsers.add_parser(
        'find', help='Batch find'
    )
    find_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    find_group = find_parser.add_mutually_exclusive_group(required=True)
    find_group.add_argument(
        '-p', '--pattern', type=str,
        help='Unix shell style pattern'
    )
    find_group.add_argument(
        '-e', '--expression', type=str,
        help='Regular expression'
    )

    # check-name options
    check_name_parser = subparsers.add_parser(
        'check-name', help='Batch check file name'
    )
    check_name_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    check_name_parser.add_argument(
        '-p', '--pattern', type=str, required=True,
        help='Pattern'
    )

    # check-integrity options
    check_integrity_parser = subparsers.add_parser(
        'check-integrity', help='Batch check file integrity'
    )
    check_integrity_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    check_integrity_parser.add_argument(
        '-i', '--input', type=str, required=True,
        help='Input hash file'
    )
    check_integrity_parser.add_argument(
        '-a', '--hash-algorithm', type=str,
        choices=[ha.value for ha in HashAlgorithm],
        default=HashAlgorithm.MD5.value,
        help='Hash algorithm. By default, use md5.'
    )

    # deduplicate options
    deduplicate_parser = subparsers.add_parser(
        'deduplicate', help='Batch deduplicate'
    )
    deduplicate_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    deduplicate_action_group = deduplicate_parser.add_mutually_exclusive_group(required=True)
    deduplicate_action_group.add_argument(
        '-a', '--action', type=str, default=DeduplicateAction.DELETE.value,
        choices=[action.value for action in DeduplicateAction],
        help='Action'
    )
    deduplicate_action_group.add_argument(
        '-d', '--dry_run', action='store_true',
        help='Dry-run mode'
    )

    # hash options
    hash_parser = subparsers.add_parser(
        'hash', help='Batch hash'
    )
    hash_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory'
    )
    hash_parser.add_argument(
        '-a', '--hash-algorithm', type=str,
        choices=[ha.value for ha in HashAlgorithm], default=HashAlgorithm.MD5.value,
        help='Hash algorithm. By default, use md5.'
    )
    hash_parser.add_argument(
        '-o', '--output', type=str,
        help='Output file'
    )
    hash_export_group = hash_parser.add_mutually_exclusive_group()
    hash_export_group.add_argument(
        '-j', '--json', action='store_true',
        help='Export as JSON file.'
    )
    hash_export_group.add_argument(
        '-c', '--csv', action='store_true',
        help='Export as CSV file.'
    )
    hash_parser.add_argument(
        '-R', '--recursive', action='store_true',
        help='Recursive traversal'
    )
    hash_condition_group = hash_parser.add_mutually_exclusive_group()
    hash_condition_group.add_argument(
        '-p', '--pattern', type=str,
        help='Pattern'
    )
    hash_condition_group.add_argument(
        '-e', '--expression', type=str,
        help='Regular expression'
    )
    hash_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # compress options
    compress_parser = subparsers.add_parser(
        'compress',
        help='Compress files based on entropy analysis'
    )
    compress_parser.add_argument(
        '-r', '--rootdir', required=True,
        help='Root directory to process'
    )
    compress_parser.add_argument(
        '-t', '--threshold', type=float, default=7.0,
        help='Entropy threshold for compression (0-8, default: 7)'
    )
    compress_parser.add_argument(
        '--remove', action='store_true',
        help='Remove original files after compression'
    )
    compress_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Show detailed processing information'
    )

    # encrypt options
    encrypt_parser = subparsers.add_parser(
        'encrypt', help='Batch encrypt files'
    )
    encrypt_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory containing files to encrypt'
    )
    encrypt_parser.add_argument(
        '-p', '--pattern', type=str,
        help='File matching pattern (e.g., \'*.txt\')'
    )
    encrypt_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # decrypt options
    decrypt_parser = subparsers.add_parser(
        'decrypt', help='Batch decrypt files'
    )
    decrypt_parser.add_argument(
        '-r', '--rootdir', type=str, required=True,
        help='Root directory containing files to decrypt'
    )
    decrypt_parser.add_argument(
        '-p', '--pattern', type=str,
        help='File matching pattern (e.g., \'*.enc\')'
    )
    decrypt_parser.add_argument(
        '-v', '--verbose', action='store_true',
        help='Verbose mode'
    )

    # convert options
    convert_parser = subparsers.add_parser(
        'convert', help='Batch convert files'
    )
    convert_parser.add_argument(
        '-s', '--src', type=str, required=True,
        help='Source directory to convert'
    )
    convert_parser.add_argument(
        '-d', '--dest', type=str, required=True,
        help='Destination directory for converted files'
    )
    convert_parser.add_argument(
        '-t', '--type', type=str, required=True,
        choices=list(CONVERSION_MAP.keys()),
        help='Conversion type: ' + ', '.join(list(CONVERSION_MAP.keys()))
    )
    convert_parser.add_argument(
        '--format', default='PNG',
        help='For image conversion, specify target format (e.g., PNG, JPEG, BMP)'
    )
    convert_parser.add_argument(
        '-r', '--recursive', action='store_true',
        help='Recursively convert files in subdirectories'
    )

    args = parser.parse_args()

    if not args.subcommand:
        print_error('Error: A subcommand is required.')
        parser.print_help()
        exit(1)

    subcommand_handlers = {
        'classify': handle_classify,
        'rename': handle_rename,
        'create': handle_create,
        'mkdir': handle_mkdir,
        'remove': handle_remove,
        'write': handle_write,
        'copy': handle_copy,
        'move': handle_move,
        'split': handle_split,
        'merge': handle_merge,
        'chmod': handle_chmod,
        'find': handle_find,
        'check-name': handle_check_file_name,
        'check-integrity': handle_check_file_integrity,
        'deduplicate': handle_deduplicate,
        'hash': handle_hash,
        'compress': handle_compress,
        'encrypt': handle_encrypt,
        'decrypt': handle_decrypt,
        'convert': handle_convert,
    }
    subcommand_handler = subcommand_handlers[args.subcommand]
    subcommand_handler(args)
    print_success('OK')


if __name__ == '__main__':
    main()
