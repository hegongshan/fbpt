#!/usr/bin/env python3
import argparse
import base64
import hashlib
import math
import os
import re
import shutil
import zipfile
from collections import defaultdict
from datetime import datetime

from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC

##################################################################
#                            Constant                            #
##################################################################
CLASSIFICATION_RULES = [
    {'type': 'time', 'granularity': 'year/month'},
    {'type': 'file_type'},
    {'type': 'size'}
]
FILE_CATEGORIES = {
    'IMAGE_EXTENSIONS': ['.jpg', '.jpeg', '.png', '.heic', '.gif'],
    'VIDEO_EXTENSIONS': ['.mp4', '.avi', '.mov', '.wmv', '.flv', '.mkv', '.m4v', '.mpg', '.mpeg'],
    'AUDIO_EXTENSIONS': ['.mp3', '.wav', '.m4a'],
    'TEXT_EXTENSIONS': ['.txt', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.pdf', '.rtf', '.odt', '.md'],
    'ARCHIVE_EXTENSIONS': ['.rar', '.7z', '.zip', '.gz', '.xz', '.tar'],
    'CODE_EXTENSIONS': ['.c', '.cpp', '.cc', '.cxx', '.cp', '.c++', '.h', '.hpp', '.hxx', '.java', '.kt',
                        '.scala', '.sc', '.py', '.rs', '.rb', '.php', '.html', '.htm', '.css', '.js', '.ts', '.sh',
                        '.bat'],
    'DATA_EXTENSIONS': ['.json', '.csv', '.dat'],
    'CONFIG_EXTENSIONS': ['.yml', '.yaml', '.xml', '.ini']
}

COLOR_RED = '\033[91m'
COLOR_GREEN = '\033[92m'
COLOR_YELLOW = '\033[93m'
COLOR_RESET = '\033[0m'


##################################################################
#                            Utility                             #
##################################################################
def check_dir(dirname):
    if not os.path.exists(dirname):
        print_error(f'Error: {dirname} does not exist!')
        exit(1)


def check_src_and_make_dest(src_dir, dest_dir):
    check_dir(src_dir)
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir)


def print_success(message):
    print(f'{COLOR_GREEN}{message}{COLOR_RESET}')


def print_warning(message):
    print(f'{COLOR_YELLOW}{message}{COLOR_RESET}')


def print_error(message):
    print(f'{COLOR_RED}{message}{COLOR_RESET}')


##################################################################
#                    Batch Processing Method                     #
##################################################################
def classify_by_time(file_path, time_format):
    mtime = os.path.getmtime(file_path)
    return datetime.fromtimestamp(mtime).strftime(time_format)


def classify_by_size(file_size):
    if file_size < 1024 * 1024:
        return '0-1M'
    if file_size < 100 * 1024 * 1024:
        return '1M-100M'
    if file_size < 1024 * 1024 * 1024:
        return '100M-1G'
    return '1G+'


def classify_by_file_type(ext):
    for category, exts in FILE_CATEGORIES.items():
        if ext in exts:
            return category.split('_')[0].lower()
    return 'others'


def batch_classify_files(rootdir, rules):
    check_dir(rootdir)

    for root, _, files in os.walk(rootdir):
        for filename in files:
            src = os.path.join(root, filename)
            if not os.path.isfile(src):
                continue

            path_parts = []
            ext = os.path.splitext(filename)[1].lower()
            size = os.path.getsize(src)

            for rule in rules:
                if rule['type'] == 'time':
                    path_parts.append(classify_by_time(src, rule['format']))
                elif rule['type'] == 'size':
                    path_parts.append(classify_by_size(size))
                elif rule['type'] == 'file_type':
                    path_parts.append(classify_by_file_type(ext))

            dest_dir = os.path.join(rootdir, *path_parts)
            os.makedirs(dest_dir, exist_ok=True)
            dest = os.path.join(dest_dir, filename)

            if src != dest:
                try:
                    shutil.move(src, dest)
                except shutil.Error:
                    base, extension = os.path.splitext(filename)
                    counter = 1
                    while True:
                        new_name = f"{base}_{counter}{extension}"
                        new_path = os.path.join(dest_dir, new_name)
                        if not os.path.exists(new_path):
                            shutil.move(src, new_path)
                            break
                        counter += 1


def batch_rename_files(rootdir, prefix, recursive=False, pattern=None, verbose=False):
    check_dir(rootdir)
    idx = 0
    for filename in os.listdir(rootdir):
        if pattern and not re.search(pattern, filename, re.IGNORECASE):
            continue
        _, ext = os.path.splitext(filename)
        new_name = os.path.join(rootdir, f'{prefix}_{idx}{ext}')
        old_name = os.path.join(rootdir, filename)
        if recursive and os.path.isdir(old_name):
            batch_rename_files(old_name, prefix,
                               recursive=recursive,
                               pattern=pattern,
                               verbose=verbose)
        if verbose:
            print(f'replace "{filename}" with "{new_name}"')
        os.rename(old_name, new_name)
        idx += 1


def batch_create_files(rootdir, count, prefix, mkdir=False, ext=None):
    if not os.path.exists(rootdir):
        os.makedirs(rootdir)
    for i in range(count):
        filename = f'{prefix}_{i}'
        if ext:
            filename += ext
        pathname = os.path.join(rootdir, filename)
        if mkdir:
            os.mkdir(pathname)
        else:
            open(pathname, 'w+').close()


def batch_remove_files(rootdir, recursive=False, pattern=None, verbose=False):
    batch_remove_files_internal(rootdir,
                                recursive=recursive,
                                pattern=pattern,
                                verbose=verbose)
    if recursive and len(os.listdir(rootdir)) == 0:
        os.rmdir(rootdir)


def batch_remove_files_internal(rootdir, recursive=False, pattern=None, verbose=False):
    check_dir(rootdir)
    for filename in os.listdir(rootdir):
        pathname = os.path.join(rootdir, filename)
        if os.path.isdir(pathname):
            if not recursive:
                continue
            if pattern and re.search(pattern, pathname):
                if verbose:
                    print(f'Remove dir: {pathname}')
                shutil.rmtree(pathname)
            else:
                batch_remove_files_internal(pathname,
                                            recursive=recursive,
                                            pattern=pattern,
                                            verbose=verbose)
        else:
            if pattern and re.search(pattern, pathname):
                if verbose:
                    print(f'Remove file: {pathname}')
                os.remove(pathname)


def batch_copy_files(src_dir, dest_dir, pattern=None, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    for filename in os.listdir(src_dir):
        if pattern and not re.search(pattern, filename):
            continue
        old_path_name = os.path.join(src_dir, filename)
        new_path_name = os.path.join(dest_dir, filename)
        if os.path.isdir(old_path_name):
            if os.path.exists(new_path_name):
                print_warning(f"Destination path '{new_path_name}' already exists")
            else:
                if verbose:
                    print(f'Copy dir: {old_path_name}')
                shutil.copytree(old_path_name, new_path_name)
        else:
            if verbose:
                print(f'Copy file: {old_path_name}')
            shutil.copyfile(old_path_name, new_path_name)


def batch_move_files(src_dir, dest_dir, pattern=None, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    any_match = False
    for filename in os.listdir(src_dir):
        if pattern and not re.search(pattern, filename):
            continue
        any_match = True
        src_path = os.path.join(src_dir, filename)
        dest_path = os.path.join(dest_dir, filename)
        if os.path.exists(dest_path):
            print_warning(f"Destination path '{dest_path}' already exists.")
        else:
            if verbose:
                print(f"Move '{src_path}' to '{dest_path}'")
            shutil.move(src_path, dest_dir)
    if not any_match:
        print(f'None of the paths match the pattern: {pattern}')


def batch_split_files_by_line(src_dir, dest_dir, line_count, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    for filename in os.listdir(src_dir):
        if filename.startswith('.'):
            continue
        pathname = os.path.join(src_dir, filename)
        with open(pathname, 'r') as file:
            lines = file.readlines()
            chunk_count = (len(lines) + line_count - 1) // line_count
            for chunk_id in range(chunk_count):
                chunk_path_name = os.path.join(dest_dir, f'{filename}_chunk{chunk_id}')
                with open(chunk_path_name, 'w+') as chunk_file:
                    chunk_file.writelines(lines[chunk_id * line_count: (chunk_id + 1) * line_count])
                    if verbose:
                        print(f'split: {pathname}, chunk id: {chunk_id}')


def batch_split_files_by_byte(src_dir, dest_dir, chunk_size, verbose=False):
    check_src_and_make_dest(src_dir, dest_dir)
    for filename in os.listdir(src_dir):
        pathname = os.path.join(src_dir, filename)
        file_size = os.path.getsize(pathname)
        chunk_number = (file_size + chunk_size - 1) // chunk_size
        with open(pathname, 'rb') as file:
            for chunk_id in range(chunk_number):
                chunk_pathname = os.path.join(dest_dir, f'{filename}_chunk{chunk_id}')
                chunk_data = file.read(chunk_size)
                with open(chunk_pathname, 'wb') as chunk_file:
                    chunk_file.write(chunk_data)
                    if verbose:
                        print(f'split: {pathname}, chunk id: {chunk_id}')


def batch_merge_files(src_dir, output_file, pattern=None, verbose=False):
    check_dir(src_dir)
    with open(output_file, 'wb') as outfile:
        for filename in os.listdir(src_dir):
            if pattern and not re.search(pattern, filename, re.IGNORECASE):
                continue
            pathname = os.path.join(src_dir, filename)
            if verbose:
                print(f'merge {pathname}')
            with open(pathname, 'rb') as infile:
                outfile.write(infile.read())


def batch_chmod(rootdir, mode, recursive=False, pattern=None, verbose=False):
    check_dir(rootdir)
    for filename in os.listdir(rootdir):
        if pattern and not re.search(pattern, filename, re.IGNORECASE):
            continue
        pathname = os.path.join(rootdir, filename)
        if recursive and os.path.isdir(pathname):
            batch_chmod(pathname, mode,
                        recursive=recursive,
                        pattern=pattern,
                        verbose=verbose)
        if verbose:
            print(f'chmod {mode} {pathname}')
        os.chmod(pathname, int(mode, 8))


def batch_find_files(rootdir, keyword):
    check_dir(rootdir)
    for filename in os.listdir(rootdir):
        pathname = os.path.join(rootdir, filename)
        if os.path.isdir(pathname):
            batch_find_files(pathname, keyword)
        else:
            if re.search(keyword, filename, re.IGNORECASE):
                print(pathname)


def batch_check_files(rootdir, pattern):
    check_dir(rootdir)
    all_match = True
    for filename in os.listdir(rootdir):
        pathname = os.path.join(rootdir, filename)
        if os.path.isdir(pathname):
            batch_check_files(pathname, pattern)
        else:
            if not re.fullmatch(pattern, filename, re.IGNORECASE):
                print_error(f'{pathname} does not match!')
                all_match = False
    if all_match:
        print_success('PASS')


def file_hash_func(filepath, block_size=65536):
    hasher = hashlib.sha256()
    with open(filepath, 'rb') as f:
        while chunk := f.read(block_size):
            hasher.update(chunk)
    return hasher.hexdigest()


def batch_deduplicate(rootdir, action='delete', dry_run=False):
    check_dir(rootdir)

    hashes = defaultdict(list)
    total_saved = 0

    for root, _, files in os.walk(rootdir):
        for filename in files:
            filepath = os.path.join(root, filename)
            if os.path.isfile(filepath):
                hash_val = file_hash_func(filepath)
                hashes[hash_val].append(filepath)

    for hash_value, files in hashes.items():
        if len(files) > 1:
            print_warning(f"发现重复文件 (SHA256: {hash_value[:8]}...)")
            keep_file = files[0]
            duplicates = files[1:]
            for dup in duplicates:
                if dry_run:
                    print(f"[模拟] 将处理: {dup}")
                    continue
                if action == 'delete':
                    size_dup = os.path.getsize(dup)
                    os.remove(dup)
                    total_saved += size_dup
                elif action == 'move':
                    dup_dir = os.path.join(rootdir, "duplicates")
                    os.makedirs(dup_dir, exist_ok=True)
                    shutil.move(dup, os.path.join(dup_dir, os.path.basename(dup)))
                print(f"已处理: {dup}")

    print_success(f"去重完成，共释放空间: {total_saved / 1024 / 1024:.2f} MB")


def calculate_entropy(file_path):
    counts = defaultdict(int)
    total = 0

    try:
        with open(file_path, 'rb') as f:
            data = f.read()
            total = len(data)
            if total == 0:
                return 0.0
            for byte in data:
                counts[byte] += 1
    except Exception as e:
        print_error(f"Error reading {file_path}: {str(e)}")
        return 0.0

    entropy = 0.0
    for count in counts.values():
        p = count / total
        entropy -= p * math.log2(p)
    return entropy


def batch_compress_files(rootdir, threshold=7.0, remove_original=False, verbose=False):
    skip_compress_extensions = FILE_CATEGORIES.get('ARCHIVE_EXTENSIONS')
    check_dir(rootdir)
    for root, _, files in os.walk(rootdir):
        for filename in files:
            src = os.path.join(root, filename)
            if not os.path.isfile(src):
                continue
            if any(filename.lower().endswith(ext) for ext in skip_compress_extensions):
                continue
            entropy = calculate_entropy(src)
            if entropy < threshold:
                if verbose:
                    print(f"Compressing {src} (entropy: {entropy:.2f})")
                zip_path = f"{src}.zip"
                try:
                    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:
                        zf.write(src, arcname=filename)
                except Exception as e:
                    print_error(f"Compression failed for {src}: {str(e)}")
                    continue
                if remove_original:
                    try:
                        os.remove(src)
                    except Exception as e:
                        print_error(f"Failed to remove {src}: {str(e)}")


def derive_key(password: str, salt: bytes) -> bytes:
    kdf = PBKDF2HMAC(
        algorithm=hashes.SHA256(),
        length=32,
        salt=salt,
        iterations=100000,
    )
    return base64.urlsafe_b64encode(kdf.derive(password.encode()))


def encrypt_file(input_path, output_path, password):
    salt = os.urandom(16)
    key = derive_key(password, salt)
    fernet = Fernet(key)
    with open(input_path, 'rb') as f:
        data = f.read()
    encrypted = fernet.encrypt(data)
    with open(output_path, 'wb') as f:
        f.write(salt + encrypted)


def decrypt_file(input_path, output_path, password):
    with open(input_path, 'rb') as f:
        salt = f.read(16)
        encrypted_data = f.read()
    key = derive_key(password, salt)
    fernet = Fernet(key)
    try:
        decrypted = fernet.decrypt(encrypted_data)
    except Exception as e:
        raise ValueError("解密失败，密码错误或文件已损坏") from e
    with open(output_path, 'wb') as f:
        f.write(decrypted)


##################################################################
#                              Main                              #
##################################################################
def main():
    parser = argparse.ArgumentParser(description='File batch processing tool (fbpt)')
    subparsers = parser.add_subparsers(dest='subcommand', help='subcommand')

    # classify options
    classify_parser = subparsers.add_parser('classify', help='Classify files into directory structure')
    classify_parser.add_argument('-r', '--rootdir', required=True, help='Root directory to process')
    classify_parser.add_argument('--rules', required=True, nargs='+',
                                 help='Classification rules (format: type[:param]) \n'
                                      'Available types: \n'
                                      '  time:strftime_format (e.g. time:%%Y/%%m) \n'
                                      '  size \n'
                                      '  file_type')

    # rename options
    rename_parser = subparsers.add_parser('rename', help='Batch rename file')
    rename_parser.add_argument('-r', '--rootdir', type=str, required=True, help='Root directory')
    rename_parser.add_argument('-p', '--prefix', type=str, required=True, help='File name prefix')
    rename_parser.add_argument('-P', '--pattern', type=str, help='Pattern')
    rename_parser.add_argument('-R', '--recursive', action='store_true', help='Recursive traversal')
    rename_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')

    # create options
    create_parser = subparsers.add_parser('create', help='Batch create file')
    create_parser.add_argument('-r', '--rootdir', type=str, required=True, help='Root directory')
    create_parser.add_argument('-c', '--count', type=int, required=True, help='Number of files/dirs')
    create_parser.add_argument('-p', '--prefix', type=str, required=True, help='File name prefix')
    create_group = create_parser.add_mutually_exclusive_group()
    create_group.add_argument('-m', '--mkdir', action='store_true', help='Create directory (default creates file)')
    create_group.add_argument('-e', '--ext', type=str, help='File extension')

    # remove options
    remove_parser = subparsers.add_parser('remove', help='Batch remove file')
    remove_parser.add_argument('-r', '--rootdir', type=str, required=True, help='Root directory')
    remove_parser.add_argument('-R', '--recursive', action='store_true', help='Recursive traversal')
    remove_parser.add_argument('-p', '--pattern', type=str, help='Pattern')
    remove_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')

    # copy options
    copy_parser = subparsers.add_parser('copy', help='Batch copy file')
    copy_parser.add_argument('-s', '--src', type=str, required=True, help='Source directory')
    copy_parser.add_argument('-d', '--dest', type=str, required=True, help='Destination directory')
    copy_parser.add_argument('-p', '--pattern', type=str, help='Pattern')
    copy_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')

    # move options
    move_parser = subparsers.add_parser('move', help='Batch move file')
    move_parser.add_argument('-s', '--src', type=str, required=True, help='Source directory')
    move_parser.add_argument('-d', '--dest', type=str, required=True, help='Destination directory')
    move_parser.add_argument('-p', '--pattern', type=str, help='Pattern')
    move_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')

    # split options
    split_parser = subparsers.add_parser('split', help='Batch split file')
    split_parser.add_argument('-s', '--src', type=str, required=True, help='Source directory')
    split_parser.add_argument('-d', '--dest', type=str, required=True, help='Destination directory')
    split_group = split_parser.add_mutually_exclusive_group(required=True)
    split_group.add_argument('-l', '--line', type=int, help='Line count')
    split_group.add_argument('-c', '--chunksize', type=int, help='Chunk Size (in byte)')
    split_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')

    # merge options
    merge_parser = subparsers.add_parser('merge', help='Batch merge file')
    merge_parser.add_argument('-s', '--src', type=str, required=True, help='Source directory')
    merge_parser.add_argument('-o', '--output', type=str, required=True, help='Output file')
    merge_parser.add_argument('-p', '--pattern', type=str, help='Pattern')
    merge_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')

    # chmod options
    chmod_parser = subparsers.add_parser('chmod', help='Batch chmod')
    chmod_parser.add_argument('-r', '--rootdir', type=str, required=True, help='Root directory')
    chmod_parser.add_argument('-m', '--mode', type=str, required=True, help='Mode')
    chmod_parser.add_argument('-R', '--recursive', action='store_true', help='Recursive traversal')
    chmod_parser.add_argument('-p', '--pattern', type=str, help='Pattern')
    chmod_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose mode')

    # find options
    find_parser = subparsers.add_parser('find', help='Batch find')
    find_parser.add_argument('-r', '--rootdir', type=str, required=True, help='Root directory')
    find_parser.add_argument('-p', '--pattern', type=str, required=True, help='Pattern')

    # check options
    check_parser = subparsers.add_parser('check', help='Batch check')
    check_parser.add_argument('-r', '--rootdir', type=str, required=True, help='Root directory')
    check_parser.add_argument('-p', '--pattern', type=str, required=True, help='Pattern')

    # reduplicate options
    reduplicate_parser = subparsers.add_parser('reduplicate', help='Batch reduplicate')
    reduplicate_parser.add_argument('-r', '--rootdir', type=str, required=True, help='Root directory')
    reduplicate_parser.add_argument('-a', '--action', type=str, default='delete', help='Action: delete or move')
    reduplicate_parser.add_argument('-d', '--dry_run', action='store_true', help='Dry-run mode')

    # compress options
    compress_parser = subparsers.add_parser('compress', help='Compress files based on entropy analysis')
    compress_parser.add_argument('-r', '--rootdir', required=True, help='Root directory to process')
    compress_parser.add_argument('-t', '--threshold', type=float, default=7.0,
                                 help='Entropy threshold for compression (0-8, default: 7)')
    compress_parser.add_argument('--remove', action='store_true',
                                 help='Remove original files after compression')
    compress_parser.add_argument('-v', '--verbose', action='store_true',
                                 help='Show detailed processing information')

    args = parser.parse_args()

    if not args.subcommand:
        print_error('Error: A subcommand is required.')
        parser.print_help()
        exit(1)

    if args.subcommand == 'classify':
        # Parse classification rules from command-line rules
        rules = []
        for rule in args.rules:
            if rule.startswith('time:'):
                _, fmt = rule.split(':', 1)
                rules.append({'type': 'time', 'format': fmt})
            elif rule == 'size':
                rules.append({'type': 'size'})
            elif rule == 'file_type':
                rules.append({'type': 'file_type'})
            else:
                print_error(f"Invalid rule: {rule}")
                exit(1)
        batch_classify_files(args.rootdir, rules)
        print_success("Classification completed successfully")
    elif args.subcommand == 'rename':
        batch_rename_files(args.rootdir, args.prefix,
                           recursive=args.recursive,
                           pattern=args.pattern,
                           verbose=args.verbose)
    elif args.subcommand == 'create':
        batch_create_files(args.rootdir, args.count, args.prefix,
                           mkdir=args.mkdir,
                           ext=args.ext)
    elif args.subcommand == 'remove':
        batch_remove_files(args.rootdir,
                           recursive=args.recursive,
                           pattern=args.pattern,
                           verbose=args.verbose)
    elif args.subcommand == 'copy':
        batch_copy_files(args.src, args.dest,
                         pattern=args.pattern,
                         verbose=args.verbose)
    elif args.subcommand == 'move':
        batch_move_files(args.src, args.dest,
                         pattern=args.pattern,
                         verbose=args.verbose)
    elif args.subcommand == 'split':
        if args.line:
            batch_split_files_by_line(args.src, args.dest,
                                      args.line, verbose=args.verbose)
        else:
            batch_split_files_by_byte(args.src, args.dest,
                                      args.chunksize, verbose=args.verbose)
    elif args.subcommand == 'merge':
        batch_merge_files(args.src, args.output,
                          pattern=args.pattern,
                          verbose=args.verbose)
    elif args.subcommand == 'chmod':
        batch_chmod(args.rootdir, args.mode,
                    recursive=args.recursive,
                    pattern=args.pattern,
                    verbose=args.verbose)
    elif args.subcommand == 'find':
        batch_find_files(args.rootdir, args.pattern)
    elif args.subcommand == 'check':
        batch_check_files(args.rootdir, args.pattern)
    elif args.subcommand == 'reduplicate':
        batch_deduplicate(args.rootdir, action=args.action, dry_run=args.dry_run)
    elif args.subcommand == 'compress':
        batch_compress_files(
            args.rootdir,
            threshold=args.threshold,
            remove_original=args.remove,
            verbose=args.verbose
        )
        print_success("Compression completed successfully")
    else:
        print_error('Error: Invalid subcommand.')


if __name__ == '__main__':
    main()
